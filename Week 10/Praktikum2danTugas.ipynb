{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5seHLBJDPVLBUOJ4iq2a7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ulfiizza27/2141720052-Machine-Learning-2023/blob/main/Week%2010/Praktikum2danTugas.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Praktikum 2**\n",
        "## Generator Teks dengan RNN\n",
        "## **Setup**\n",
        "\n",
        "#### **Import TensorFlow**"
      ],
      "metadata": {
        "id": "OVFV4ecI7AAt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Vyi16pCn67Rt"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Download Dataset Shakespeare**\n",
        "\n",
        "Sesuaikan dengan lokasi data yang Anda punya."
      ],
      "metadata": {
        "id": "Wldfi_dI2RF0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXAee2B29ods",
        "outputId": "4d9757f6-d721-487d-9e40-cea5ac68beb3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Load Data**"
      ],
      "metadata": {
        "id": "0URLcO6l2mKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcTfMflZ2bJd",
        "outputId": "7abf8b46-54b6-412c-8577-0b2bba45822f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EyRGKeRq2sKa",
        "outputId": "50748cb2-991d-488c-87cd-355f2c2d02d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLTAhu-52t9C",
        "outputId": "69f4e6f7-7025-49c1-94b2-3b667979286e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Olah Teks**\n",
        "Vectorize Teks\n",
        "\n",
        "Sebelum training, Anda perlu mengonversi string menjadi representasi numerik. tf.keras.layers.StringLookup dapat mengubah setiap karakter menjadi ID numerik. Caranya adalah teks akan dipecah menjadi token terlebih dahulu."
      ],
      "metadata": {
        "id": "MT2v0Y7V2yG9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHhkK4nK2vyq",
        "outputId": "52d0897a-64e0-46e6-b2cc-b1f3428d575d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "sekarang buat tf.keras.layers.StringLookup layer:"
      ],
      "metadata": {
        "id": "Zv5wr8RH4Dls"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "FxkXJZQF3FD_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "perintah diatas mengconvert token menjadi id"
      ],
      "metadata": {
        "id": "TCT9AszK4Ozt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n23_IuNk4GGQ",
        "outputId": "b2e3d423-403c-4a36-fbbb-75e14b8acbe8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Karena tujuan tutorial ini adalah untuk menghasilkan teks, penting juga untuk membalikkan representasi ini. Untuk ini Anda dapat menggunakan kode tf.keras.layers.StringLookup(..., invert=True).\n",
        "\n",
        "Catatan: pada kode ini, daripada meneruskan kosakata asli yang dihasilkan dengan diurutkan(set(teks)) gunakan metode get_vocabulary() dari tf.keras.layers.StringLookup sehingga token [UNK] disetel dengan cara yang sama."
      ],
      "metadata": {
        "id": "gsx6H9Zk4fBr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "-rJpJD-y4VRp"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lapisan ini mengconvert kembali karakter dari vektor ID, dan mengembalikannya sebagai karakter tf.RaggedTensor:"
      ],
      "metadata": {
        "id": "Oy3KIXS94lfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwTGa4Oe4ipm",
        "outputId": "8ed2236a-326e-495c-9407-7d2994a027ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Anda dapat menggunakan tf.strings.reduce_join untuk menggabungkan kembali karakter menjadi string."
      ],
      "metadata": {
        "id": "HCbsOacX4sbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KrMXMwbX4pLr",
        "outputId": "d052c183-2d10-4a28-f146-3c9fb620b391"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOzPHWyx4yCA",
        "outputId": "90569a96-b65f-4f8c-eece-0c3867c42005"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "6ql6PXmf40y3"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Membuat Trianing Set dan Target**\n",
        "Selanjutnya bagilah teks menjadi contoh sequence. Setiap masukan sequence akan berisi karakter seq_length dari teks. Untuk setiap masukan sequence, target prediksi berisi teks dengan panjang yang sama, hanya digeser satu karakter ke kanan. Jadi, bagi teks menjadi beberapa bagian seq_length+1. Misalnya, seq_length adalah 4 dan teks kita adalah \"Hello\". Urutan masukannya adalah \"Hell\", dan urutan targetnya adalah \"ello\". Untuk melakukan ini, pertama-tama gunakan fungsi tf.data.Dataset.from_tensor_slices untuk mengonversi vektor teks menjadi aliran indeks karakter."
      ],
      "metadata": {
        "id": "G-YLU7_z6FLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZgSaJGa48U8",
        "outputId": "027baaed-b47e-46ce-b626-2869d0efc75c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "avtSXJGu6MZI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMKPHLda6b7U",
        "outputId": "06dfeb71-1ec2-4e00-aa9f-ba5549837f6b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "E0Xne_zT6feB"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Metode batch memungkinkan Anda dengan mudah mengonversi karakter individual ini menjadi urutan ukuran yang diinginkan."
      ],
      "metadata": {
        "id": "cX0v8x4wJjuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "id": "JVHN783c6qnV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "847126c7-c059-4cd3-9d95-72c5afb6e72f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "akan lebih mudah untuk melihat apa yang dilakukan jika Anda menggabungkan token kembali menjadi string:"
      ],
      "metadata": {
        "id": "7T7mR0KoJpLJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GAkAUYROJnRY",
        "outputId": "afd26ab3-5f28-4530-91e4-3c0512c51f02"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk pelatihan, Anda memerlukan kumpulan data pasangan (input, label). Dimana input dan label merupakan urutan. Pada setiap langkah waktu, inputnya adalah karakter saat ini dan labelnya adalah karakter berikutnya. Berikut adalah fungsi yang mengambil urutan sebagai masukan, menduplikasi, dan menggesernya untuk menyelaraskan masukan dan label untuk setiap langkah waktu:"
      ],
      "metadata": {
        "id": "UZFKet5zJySZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "vW2IHsRQJrHn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJefbsGdJ_Bb",
        "outputId": "21502117-4674-42e5-9a70-ae5a8eef87f0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "qiDA-wtSKR5j"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7oeODtoKVBR",
        "outputId": "393d725a-8bfb-408d-a208-0df4b11f7414"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Membuat Batch Training**\n",
        "\n",
        "Anda menggunakan tf.data untuk membagi teks menjadi sequence yang dapat diatur. Namun sebelum memasukkan data ini ke dalam model, Anda perlu mengacak data dan mengemasnya ke dalam batch."
      ],
      "metadata": {
        "id": "qwhdtaD0KvOJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hrC21tBKgWJ",
        "outputId": "eb13fb43-8783-47a6-97a0-592351f7e490"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Buat Model**\n",
        "\n",
        "Bagian ini mendefinisikan model sebagai subkelas keras.Model (untuk lebih detilnya, lihat Making new Layers and Models via subclassing).\n",
        "\n",
        "Model yang kita bangun memiliki 3 lapisan neural network :\n",
        "\n",
        "- tf.keras.layers.Embedding: Lapisan masukan. Tabel pencarian yang dapat dilatih yang akan memetakan setiap karakter-ID ke vektor dengan dimensi embedding_dim;\n",
        "- tf.keras.layers.GRU: lapisan RNN dengan ukuran unit=rnn_units (Anda juga dapat menggunakan lapisan LSTM di sini.)\n",
        "- tf.keras.layers.Dense: Lapisan keluaran, dengan keluaran vocab_size. Ini menghasilkan satu logit untuk setiap karakter dalam kosakata. Ini adalah log kemungkinan setiap karakter menurut model."
      ],
      "metadata": {
        "id": "c2dHShy6K4M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "FPSYbiNNK0Nb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "WgQYwETLLHqY"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "2CHtpmgvLJXO"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Uji Model**\n",
        "\n",
        "Coba jalankan model dan cek apakah sidah sesuai dengan output\n",
        "pertama, cek bentuk dari output"
      ],
      "metadata": {
        "id": "MElbnfSULOMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jvIe6HuLKrg",
        "outputId": "7ed11c7e-281b-41cb-ea7e-612f6a395647"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dalam contoh di atas, panjang urutan masukan adalah 100 tetapi model dapat dijalankan pada masukan dengan panjang berapa pun:"
      ],
      "metadata": {
        "id": "wFRhf-s9Li9p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cosgf7MkLUCW",
        "outputId": "8aaab544-2a51-4a24-9447-98f6cfbe6ed1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Untuk mendapatkan prediksi aktual dari model, Anda perlu mengambil sampel dari distribusi keluaran, untuk mendapatkan indeks karakter aktual. Distribusi ini ditentukan oleh logit pada kosakata karakter. Catatan: Penting untuk mengambil sampel dari distribusi ini karena mengambil argmax dari distribusi tersebut dapat dengan mudah membuat model terjebak dalam infinote loop. Cobalah untuk contoh pertama di batch:"
      ],
      "metadata": {
        "id": "g3Jr0uSMLpE5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "AgTBT6gZLl0g"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hal ini memberi kita, pada setiap langkah waktu, prediksi indeks karakter berikutnya:"
      ],
      "metadata": {
        "id": "aizbuP9-LywP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-MMQEQULq5p",
        "outputId": "13554b73-05a5-4ec9-e7c5-9874198668de"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([40, 38, 25, 23, 58, 38, 35, 45, 22, 39,  9, 57, 48,  6, 60,  4,  0,\n",
              "       26, 10, 31, 60, 10, 42,  3, 37, 45, 64, 12, 58, 56, 32, 20, 10, 63,\n",
              "       36, 59, 29, 11, 51, 55, 54, 28, 52, 54, 60, 61, 64,  0, 11, 44, 25,\n",
              "        1, 19, 51, 38,  7,  2,  3, 24, 28, 41, 22,  2, 21, 23,  5, 20,  7,\n",
              "       30,  8, 37, 62, 30, 34, 38, 65, 56,  9,  3, 10, 40,  2, 63, 41,  3,\n",
              "       14, 64, 13, 35, 65, 13, 59, 43, 55, 40, 21, 45, 55, 63, 31])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dekode kode berikut untuk melihat teks yang diprediksi oleh model tidak terlatih ini:"
      ],
      "metadata": {
        "id": "ltPcMzh1L2Zb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZvFD3HPL0ot",
        "outputId": "6ee6f3a4-71c7-4cf0-fc0a-42e137c90f2e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b'ors are packed:\\nWhere bloody Tybalt, yet but green in earth,\\nLies festering in his shroud; where, as'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"aYLJsYVfIZ.ri'u$[UNK]M3Ru3c!Xfy;sqSG3xWtP:lpoOmouvy[UNK]:eL\\nFlY, !KObI HJ&G,Q-XwQUYzq.!3a xb!Ay?Vz?tdpaHfpxR\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Train Model**\n",
        "\n",
        "Pada titik ini permasalahan dapat dianggap sebagai permasalahan klasifikasi standar. Permasalahan dapat disimpulkan dengan : Berdasarkan status RNN sebelumnya, dan masukan langkah kali ini, prediksi kelas karakter berikutnya.\n",
        "\n",
        "##### **Tambahan optimizer dan fungsi loss**\n",
        "\n",
        "loss function tf.keras.losses.sparse_categorical_crossentropy standar berfungsi dalam kasus ini karena diterapkan di seluruh dimensi terakhir prediksi. Karena model Anda mengembalikan logits, Anda perlu mengatur flag from_logits."
      ],
      "metadata": {
        "id": "KrA40jq_MMWb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "3-OgOXJ_L4EW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAhbASNNMYN9",
        "outputId": "b3c135eb-4ace-4b77-bdad-d7bc63dbbcd4"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.1889343, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model yang baru diinisialisasi tidak boleh terlalu yakin dengan dirinya sendiri, semua log keluaran harus memiliki besaran yang sama. Untuk mengonfirmasi hal ini, Anda dapat memeriksa bahwa eksponensial dari loss rata-rata harus kira-kira sama dengan ukuran kosakata. Loss yang jauh lebih tinggi berarti model tersebut yakin akan jawaban yang salah, dan memiliki inisialisasi yang buruk:"
      ],
      "metadata": {
        "id": "ft2xL5PpMiA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcP8dG0FMf__",
        "outputId": "96ac9fc2-2202-4252-d452-3a8f81ce246d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65.95247"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Konfigurasikan prosedur pelatihan menggunakan metode tf.keras.Model.compile. Gunakan tf.keras.optimizers.Adam dengan argumen default dan fungsi loss."
      ],
      "metadata": {
        "id": "ucY5x2UIMqBC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "xfAmAFnVMoT_"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Konfigurasi Checkpoints**\n",
        "\n",
        "Gunakan tf.keras.callbacks.ModelCheckpoint untuk memastikan bahwa checkpoint disimpan selama pelatihan:"
      ],
      "metadata": {
        "id": "ay9hZN8PMx_2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "itCnYC8vMsOn"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Lakukan Proses Training**\n",
        "\n",
        "Agar waktu pelatihan tidak terlalu lama, gunakan 10 epoch untuk melatih model. Di Colab, setel runtime ke GPU untuk pelatihan yang lebih cepat."
      ],
      "metadata": {
        "id": "bmF8f0hnM4Y1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 20"
      ],
      "metadata": {
        "id": "2BFz0JSEM2uT"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElUOAmuIM8mn",
        "outputId": "898cba3b-700b-4447-9b4f-5592858e2f52"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 20s 66ms/step - loss: 2.7301\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.9929\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 12s 57ms/step - loss: 1.7118\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.5480\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 1.4487\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 1.3806\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.3281\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2831\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2424\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.2028\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1633\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.1223\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 1.0788\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 12s 58ms/step - loss: 1.0334\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9852\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9359\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8825\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.8313\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7804\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Generate Teks**\n",
        "\n",
        "Setiap kali Anda memanggil model, Anda memasukkan beberapa teks dan state internal. Model mengembalikan prediksi untuk karakter berikutnya dan state barunya. Masukkan kembali prediksi dan state ke model untuk terus menghasilkan teks."
      ],
      "metadata": {
        "id": "7Prg-zS-NIye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "QbjCfkh6NClI"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "lgIOzXbRNZvk"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jalankan secara berulang untuk menghasilkan beberapa teks. Melihat teks yang dihasilkan, Anda akan melihat model mengetahui kapan harus menggunakan huruf besar, membuat paragraf, dan meniru kosakata menulis seperti Shakespeare. Karena sedikitnya jumlah epoch pelatihan, model belum belajar membentuk kalimat runtut."
      ],
      "metadata": {
        "id": "ocOYwlQhNfAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrmIt5B_Nh0t",
        "outputId": "13a7a847-e198-48ca-e1b5-7ccdd037f5e7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Thus with a goodly spirit?\n",
            "\n",
            "PALIX:\n",
            "But hast thou so o'er the marse.\n",
            "The fruits of raping scorn, or so my heart.\n",
            "\n",
            "KATHARINA:\n",
            "A sire of mildness, will not go.\n",
            "\n",
            "POLIXENES:\n",
            "O, teach your hands and comfort would\n",
            "Hurl'd at our coming hither.\n",
            "Therefore, sir, would you remember\n",
            "And morit in the entirman'd hap\n",
            "As hope of thine; or be, like coilign,\n",
            "Would be enkied against which spirit\n",
            "Is not to be the sailors. Though my head and reconciled\n",
            "To pray for no wert world she loves: and have an ends\n",
            "shade out for prayers so oft nor anger.\n",
            "\n",
            "KING LEWIS XI:\n",
            "Why, Warwick, when you part,\n",
            "As well as I had servent to heal;\n",
            "But Warwick and our company at Bianca,\n",
            "The princes his wailt which time as fasting nur,\n",
            "Durst be a pilchment perdilous than my--uncreas'd you\n",
            "With being fined, or do I with deed with others' gifter,\n",
            "Our soldiers sound in secona him,\n",
            "Your son will not vex your precious rest.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "\n",
            "FRIAR LAURENCE:\n",
            "Have you of your daughter's joints than done with me,\n",
            "But out another wearing awa \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.232938528060913\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hal termudah yang dapat Anda lakukan untuk meningkatkan hasil adalah dengan melatihnya lebih lama (coba EPOCHS = 30). Anda juga dapat bereksperimen dengan string awal yang berbeda, mencoba menambahkan lapisan RNN lain untuk meningkatkan akurasi model, atau menyesuaikan parameter suhu untuk menghasilkan prediksi yang kurang lebih acak.\n",
        "\n",
        "Jika Anda ingin model menghasilkan teks lebih cepat, hal termudah yang dapat Anda lakukan adalah membuat teks secara batch. Pada contoh di bawah, model menghasilkan 5 keluaran dalam waktu yang hampir sama dengan waktu yang dibutuhkan untuk menghasilkan 1 keluaran di atas."
      ],
      "metadata": {
        "id": "TXkD3A5tNl6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFy8hEBINoXb",
        "outputId": "5827c509-5e7e-41b0-e7bb-aef3c0dc26ad"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThat were considered, impartial roods\\nInserfection of such a soft heart-place!\\n\\nGLOUCESTER:\\nI' loss a scorn or bone or powers.\\n\\nLEONTES:\\nThou hast some accused in:\\nI love her.\\n\\nPETRUCHIO:\\nYour art thee! my brother flatter me,\\nSo I pray thee.\\n\\nROMEO:\\nIs the about that I have well discharged:\\nThy farder news be king, but that false gape\\nscurvy by which with cause be a word.\\n\\nCAPULET:\\nWhich is his name?\\n\\nCORIOLANUS:\\nLovel! what of that?\\n\\nMENENIUS:\\nThere's mady great Hortensio.\\n\\nMIRANDA:\\nIn such a one\\nmore precious as your honour's pardon.\\n\\nMENENIUS:\\nHas he heard this forward of my fow?\\n\\nLEONTES:\\nBefore so prove a loven, so be life?\\nI could have slain my child of my youth;\\nFor your heady sprawn and bows with nime and call him\\nheard of!\\nLet it be inhorant too repeal;\\nSo o'e worth care is just, and then\\nUnstainted Licert with me to me?\\n\\nHORTENSIO:\\nBut as ston'd in the middle own perassion.\\nThe entertainment should this newly ledging,\\nBecause she could hide hence forth forbotion;\\nI do not ke\"\n",
            " b\"ROMEO:\\nThe tongue as you them never heard,\\nTo some the children yet unbow'd-breath'd crown of wilf.\\n\\nDUKE OF YORK:\\nHo! it is my commony.\\n\\nPedant:\\nSweet Didow.\\n\\nANTIGONUS:\\nMy husband!\\nMake heavy an oul gentle for necessacy,\\nDeserve not so commit meet the town.\\n\\nMessenger:\\nSuch my revenges that I intend I wore\\nDesire to look on.\\n\\nTRANIO:\\nCome, pentally; come, towards Marcius shall follow\\nAs the sweet infeit of days; thou one prison\\ncarrares.;\\nBut, speed as much before it. I will not say that Kate\\nBut only in this character, weighing lever.\\nWhat stars chide shif with wolves of like haste.\\n\\nDUCHESS OF YORK:\\nO, let it stial, adieu; commend me to my cousin;\\nBut whether 'twas time: whatsoe'er I strike?\\nThy favour act of parasing Bolingbroke\\nAre as one of you alasp, yet still some other\\nCastan my heart with an ill coward, which he\\nis, and neither follows to reliver.\\nIf you have ever you know, I think!\\n\\nShepherd:\\nLet me this it stars in all the rest faced locks are night,\\nI'll take them offer no nobl\"\n",
            " b\"ROMEO:\\nThe old curbshamel.\\n\\nVINCENTIO:\\nI pray the Eurting Henry!\\n\\nCaptain:\\nMore my lord where shall we part in colour.\\n\\nBARNARDINE:\\nI'll swear, how he will take out so blind confound\\nWhat loss a one made peace.\\n\\nNurse:\\nBear hence these your highness come to me,\\nThat I'll pincon things should not be pardon'd,\\nI heard you speak a word or power, but that I oak.\\n\\nBoth:\\nMy lord!\\n\\nKING RICHARD II:\\nUncle, what offend by the rest,\\nA man of bleeding-morrow?\\nNe'er a mead worn with the other for his death,\\nWhere to thyself untall'd, you are home.\\n\\nLADY CAPULET:\\nI am slain.\\n\\nDUKE OF AUMERLE:\\nMy lord, then did spit for my brother.\\n\\nKING EDWARD IVa'\\nUnder our pleasure and our general good,\\nTo make me come, I had remembered my days!\\nCome, come, thou arm rock, sir, in happy haste.\\nThink you the man I'll cross this gentleman's child;\\nMy sovereign liege, I will not yours.\\n\\nHASTINGS:\\nFavousing horse is forfally.\\n\\nGONZALO:\\nHappy, by your leave must go bach.\\nCome, such gentle Near and scold:\\nIf he be play, and t\"\n",
            " b\"ROMEO:\\nI would to-morrow mortal praise to be\\nBless of his charge.\\n\\nCLAUDIO:\\nThen let me know you may; I say;\\nAnd I with soldiers at your pleasure?\\nO, how she want thou even too, good prince is,\\nSet down the advantage of the hulticy burden.\\nBut what say you, my lord?\\n\\nKING RICHARD II:\\nThen muster up and welcome home.\\n\\nVIRGILIA:\\nI do, another mad-morrow?\\n\\nSecont Murderer:\\nDo, little weep; what with nothing ere he two earsh you\\nmake, take him too much upon this sudden.\\n\\nWARWICK:\\nEngly Warwick, sir; who set this in the ruee.\\n\\nGLOUCESTER:\\nNo doubt, no deceit.\\n\\nJULIET:\\nGently bear them thanks you receive\\nYour fresh an angry arms, dust once noted.\\n\\nEDWARD:\\nUpon a gentleman of mine shall be\\nOf mortal touch of preyza and gone,\\nHow in a mile for Claudio's death,\\nIf brist me preparding by the earld's virtue\\nTo seek out death--love's life's dear,\\nAnd show myself and motions, which show\\nRichmond in this outward, how your care\\nGot we disorder this wrong-hard-faced begot;\\nWhen Those grave's toaded blunts b\"\n",
            " b\"ROMEO:\\nThese piteous stooping sorrows may be sworn\\nIs takent and sorrow for the fresh-hated.\\n\\nLEONTES:\\nI spirit it\\nexpecting that our weapon, to a chafterous teats?\\n\\nRICHARD:\\nA cold and nature, being swear, if thou'lt fing\\nher fair demearest: we art thou mad?\\n\\nJULIET:\\nWhat's this?\\n\\nLUCENTIO:\\nI get a gassing lip on him; and his\\nmight be silent. Heartiest all I call'd them to the gods.\\n\\nSICINIUS:\\nYou are welcome.\\n\\nANTONIO:\\nOf but wast thou, that I reside her in her husband\\nBe till casting thee with doing;\\nAnd by their break and love, that see'st, and show'd\\nUnto meet and happy by thy birth,\\nNo such a usacled, alter in me\\nthan thy husband more incured me; cay you read\\nnot be so power: if a grudge Please you the report\\nThat all as long as kindness would quickly give\\nEdward thy love, cannot too me;\\nThat she was woe were now, and artisent;\\nAnd therein my name shall be mine own him,\\nAnd so your duke committed to the queen,\\nOut of thy name; and, be not so done,\\nThat my heart is not corn to thee.\\n\\nCA\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.958622932434082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ekspor Model Generator**\n",
        "\n",
        "Model satu langkah ini dapat dengan mudah disimpan dan digunakan kembali, memungkinkan Anda menggunakannya di mana pun tf.saved_model diterima."
      ],
      "metadata": {
        "id": "_j2y-MD3NqpR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WezK6JGMNu0k",
        "outputId": "cadcb408-aff5-463b-bd39-3c2e9df92379"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7f1bc58fb970>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M0qgPVVNwac",
        "outputId": "f66a0891-e020-423c-efef-5e3411adf0e8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "There are the roubbly of this knightly plague:\n",
            "Thou please to her good deeds on me,\n",
            "But kill no hop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tugas Praktikum**\n",
        "\n",
        "1. Jalankan Model dan hitung loss dengan tf.GradientTape.\n",
        "2. Hitung update dan terapkan pada model dengan optimizer"
      ],
      "metadata": {
        "id": "71F8WlHj9Rol"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "      inputs, labels = inputs\n",
        "      with tf.GradientTape() as tape:\n",
        "          predictions = self(inputs, training=True)\n",
        "          loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ],
      "metadata": {
        "id": "hkG-l3wA-XRF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode diatas menerapkan train_step method sesuai dengan  Keras' train_step conventions. Ini opsional, tetapi memungkinkan Anda mengubah perilaku langkah pelatihan dan tetap menggunakan keras Model.compile and Model.fit methods."
      ],
      "metadata": {
        "id": "SpuIQK_P_H1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "2ODn7wAe-0MP"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "Ys_lqj8q_N0K"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kW6U4xeF_S_C",
        "outputId": "29dcc150-f92c-4dbf-e2ad-0b2fd8716f83"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 16s 61ms/step - loss: 2.7012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7f1cc9008b50>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri:"
      ],
      "metadata": {
        "id": "fJcFlxUB_XcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    mean.reset_states()\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HCOwu3F_Uip",
        "outputId": "c9715430-d357-43b2-cc8b-908b7f760841"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1731\n",
            "Epoch 1 Batch 50 Loss 2.0534\n",
            "Epoch 1 Batch 100 Loss 1.9441\n",
            "Epoch 1 Batch 150 Loss 1.8117\n",
            "\n",
            "Epoch 1 Loss: 1.9754\n",
            "Time taken for 1 epoch 15.32 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.7678\n",
            "Epoch 2 Batch 50 Loss 1.7648\n",
            "Epoch 2 Batch 100 Loss 1.6926\n",
            "Epoch 2 Batch 150 Loss 1.6094\n",
            "\n",
            "Epoch 2 Loss: 1.6973\n",
            "Time taken for 1 epoch 12.91 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.6010\n",
            "Epoch 3 Batch 50 Loss 1.6165\n",
            "Epoch 3 Batch 100 Loss 1.5140\n",
            "Epoch 3 Batch 150 Loss 1.4689\n",
            "\n",
            "Epoch 3 Loss: 1.5414\n",
            "Time taken for 1 epoch 12.29 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4919\n",
            "Epoch 4 Batch 50 Loss 1.4746\n",
            "Epoch 4 Batch 100 Loss 1.4436\n",
            "Epoch 4 Batch 150 Loss 1.4646\n",
            "\n",
            "Epoch 4 Loss: 1.4458\n",
            "Time taken for 1 epoch 11.84 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3733\n",
            "Epoch 5 Batch 50 Loss 1.3862\n",
            "Epoch 5 Batch 100 Loss 1.3866\n",
            "Epoch 5 Batch 150 Loss 1.3171\n",
            "\n",
            "Epoch 5 Loss: 1.3795\n",
            "Time taken for 1 epoch 12.11 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.2915\n",
            "Epoch 6 Batch 50 Loss 1.2892\n",
            "Epoch 6 Batch 100 Loss 1.3508\n",
            "Epoch 6 Batch 150 Loss 1.3145\n",
            "\n",
            "Epoch 6 Loss: 1.3277\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2878\n",
            "Epoch 7 Batch 50 Loss 1.2974\n",
            "Epoch 7 Batch 100 Loss 1.2992\n",
            "Epoch 7 Batch 150 Loss 1.3187\n",
            "\n",
            "Epoch 7 Loss: 1.2843\n",
            "Time taken for 1 epoch 11.28 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2075\n",
            "Epoch 8 Batch 50 Loss 1.2486\n",
            "Epoch 8 Batch 100 Loss 1.2666\n",
            "Epoch 8 Batch 150 Loss 1.2141\n",
            "\n",
            "Epoch 8 Loss: 1.2422\n",
            "Time taken for 1 epoch 11.71 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1825\n",
            "Epoch 9 Batch 50 Loss 1.2066\n",
            "Epoch 9 Batch 100 Loss 1.2039\n",
            "Epoch 9 Batch 150 Loss 1.2435\n",
            "\n",
            "Epoch 9 Loss: 1.2033\n",
            "Time taken for 1 epoch 11.93 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1362\n",
            "Epoch 10 Batch 50 Loss 1.1445\n",
            "Epoch 10 Batch 100 Loss 1.1558\n",
            "Epoch 10 Batch 150 Loss 1.1572\n",
            "\n",
            "Epoch 10 Loss: 1.1632\n",
            "Time taken for 1 epoch 11.58 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "### **Jawab**\n",
        "\n",
        "Pada **tf.GradientTape** digunakan untuk melacak gradien dari operasi-operasi yang terjadi dalam TensorFlow, terutama saat menghitung gradien loss terhadap parameter-parameter model selama pelatihan. Dan digunakan sebagai metode pelatihan khusus untuk menghitung gradien dan mengontrol perhitungan gradien.\n",
        "\n",
        "Sedangkan pada **tf.keras.Model** digunakan untuk mendefinisikan arsitektur model secara deklaratif, mengelola model, serta melatih dan mengevaluasi model. Dan pada tf.keras.Model juga digunakan untuk mendefinisikan arsitektur model dan mengatur aliran data melalui model, serta digunakan dalam proses pelatihan model secara umum.\n"
      ],
      "metadata": {
        "id": "4xNYvM4PHJan"
      }
    }
  ]
}